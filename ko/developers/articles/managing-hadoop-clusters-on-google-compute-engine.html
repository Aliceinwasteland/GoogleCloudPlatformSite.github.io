<!DOCTYPE html>
<html devsite="">
<head>
<title>Big Data Articles &amp; Tutorials — Google Cloud Platform</title>
<meta name="description" content="Read about technical big data articles and solutions with Google Cloud Platform. Big data articles unpack BigQuery data analytics, Hadoop clusters and more.">
<meta name="hide_page_heading" value="true">
<meta name="full_width" value="true">
<meta name="top_category" value="developers">
<meta name="subcategory" value="articles">
<meta name="viewport" content="initial-scale=1, minimum-scale=1, width=device-width">
<link rel="apple-touch-icon" href="/_static/images/apple-touch-icon.png">
<link href="//fonts.googleapis.com/css?family=Open+Sans:400italic,300,400,600,700" rel="stylesheet">
<link href="/css/default.css" rel="stylesheet"><!--[if lt IE 9]>
    <link rel="stylesheet" media="screen" href='/c/cp-ie.css'>
    <![endif]-->
<script src="/js/floodlight.js">
</script>
</head>
<body>
<div id="maia-main" class="cp-article">
<div class="maia-cols">
<div class="maia-col-9">
<div>
<div style="float:right">
<div class="g-plusone"></div>
</div>
<h1 class="title">Google Compute Engine에서 Hadoop 클러스터 관리</h1>
</div>
<div class="cp-article-tutorial">
<h2>소개</h2>
<p>Google Compute Engine은 Hadoop 실행을 위한 <a href="http://www.mapr.com/press-release/mapr-and-google-compute-engine-set-new-world-record-for-hadoop-terasort">검증된</a> 플랫폼입니다. 일관성 있는 고성능 가상 머신과 이러한 가상 머신을 서로 연결해 주고 Google Cloud Platform의 다른 서비스에도 연결해 주는 지연 시간이 낮은 고대역폭 네트워크와 결합하여 클라우드 기반 데이터 처리 아키텍처를 위한 단단한 토대를 제공해 줍니다. 요청 시 다수의 가상 머신을 신속하게 인스턴스화할 수 있는 기능과 분당 가격 책정 서비스를 제공하므로 Compute Engine은 즉석 클러스터를 가동 및 중지하는 데 매우 적합하며 비용 면에서도 효율적입니다. 또한 Google의 전 세계적인 네트워크 성능과 함께 Compute Engine의 고급 라우팅 및 데이터 암호화 기능을 통해 안전하고 강력한 하이브리드 솔루션을 만들 수 있습니다.</p>
<p>이 문서에서는 Google Compute Engine에 자체적으로 Hadoop을 설치하려는 개발자를 위한 몇 가지 기술과, 장기간 가동되는 클러스터를 처리하고 여러 영역에서 작업할 때 발생할 수 있는 엄선된 시나리오(영역 간 마이그레이션, 재해 복구 및 고가용성)를 제시합니다. 다른 솔루션이 분명히 존재하고 이를 특정 상황에 적용하는 것이 더 적합할 수 있으므로, 이러한 목록이 완전하거나 항상 권장되는 것은 아닙니다. 하지만 이 문서는 Hadoop 실행에 있어 Compute Engine을 강력한 플랫폼으로 만들어주는 동일한 일부 기능을 <a href="https://developers.google.com/compute/docs/gcutil/"><span class="code">gcutil</span></a> 명령줄 유틸리티와 함께 사용했을 경우, 어떻게 Hadoop 클러스터 관리 솔루션을 위한 토대를 제공해 줄 수 있는지를 보여주는 데 목적이 있습니다.</p>
<h2>시나리오</h2>
<p>이 섹션에서 제시된 시나리오는 모두 부정적인 환경 관련 사건(예: 영역에 예상치 못한 정전이 발생하는 경우)에 직면했을 때 고려 중인 클러스터가 최대한 잘 보존되어야 한다는 전제를 바탕으로 하고 있습니다. 이는 작업 파이프라인 및 스트림 처리를 지원하는 데 사용되는 장기간 클러스터에서 발생하기 힘든 상황이지만 그렇다고 가능성이 없는 것은 아닙니다. 요청형 클러스터<sup><a href="#ftnt1" name="ftnt_ref1" id="ftnt_ref1">[1]</a></sup>는 관리할 필요가 거의 없으므로, 일반적으로 이 문서의 범위에 포함되지 않습니다. 또한 제시된 시나리오는 클러스터에서 사용하고 생성하는 데이터가 자체 관리되는 HDFS(Hadoop 분산 파일 시스템)에서 지속적으로 유지되어야 한다고 가정합니다.<sup><a href="#ftnt2" name="ftnt_ref2" id="ftnt_ref2">[2]</a></sup></p>
<p>관리자는 유지관리 이벤트에 맞게 계획을 수립하고 클러스터 가용성을 관리할 수 있습니다. 하지만 예상치 못한 상황이 발생할 경우 현재 처리 중인 작업이 영향을 받을 가능성이 높습니다. 구체적은 결과는 Hadoop <a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support">배포판</a>에 따라 다르지만, 대부분의 경우 이러한 작업은 재개되는 대신 다시 시작되며, 작업을 다시 제출해야 할 수도 있습니다.</p>
<h3>영역 마이그레이션</h3>
<p>특정 시점에 클러스터를 한 영역에서 다른 영역으로 이동 또는 복제하는 것이 바람직하거나 <a href="https://developers.google.com/compute/docs/faq#maintenance">필요</a>할 수 있습니다. 이 섹션에서는 이러한 작업을 수행할 수 있는 서로 다른 두 가지 시나리오를 제시합니다. 첫 번째 시나리오에서는 운영이 복잡해지는 대신, 클러스터의 비가용성을 최소화하여 MapReduce 작업을 수락하고 실행하는 데 목적이 있습니다. 두 번째 시나리오에서는 매우 간단한 관리 방식을 얻는 대신, 비가용성이 추가적으로 발생합니다.</p>
<h4>시나리오 1: 비가용성 최소화</h4>
<p>보통 클러스터는 지속적으로 사용할 수 있습니다(특히 클러스터가 한 개인 경우). 하지만 사용자가 작업을 제출할 수 없는 기간이 있으면 큰 손실로 이어질 수 있습니다. 첫 번째 접근 방식에서는 기존 배포에서 이러한 기간을 최대한 줄이려고 합니다.<sup><a href="#ftnt3" name="ftnt_ref3" id="ftnt_ref3">[3]</a></sup> 먼저 새 <a href="http://wiki.apache.org/hadoop/DataNode">DataNodes</a> 및 <a href="http://wiki.apache.org/hadoop/TaskTracker">TaskTrackers</a>(또는 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN NodeManagers</a>)가 도착 영역의 클러스터에 추가됩니다. 그런 다음 마스터 노드(예: <a href="http://wiki.apache.org/hadoop/NameNode">NameNode</a>, <a href="http://wiki.apache.org/hadoop/JobTracker">JobTracker</a> 또는 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN ResourceManager</a>) 또한 전환됩니다. 이 방식을 사용하면 클러스터의 비가용성이 대략 소스 영역의 마스터 서비스 중단부터 도착 영역의 서비스 시작까지의 간격으로 정의됩니다.</p>
<h5>고려사항</h5>
<ol>
<li>DataNode는 HDFS에 <a href="https://developers.google.com/compute/docs/disks">영구 디스크</a>를 사용합니다. 영구 디스크는 인스턴스 기간 이상의 데이터 지속성을 제공합니다. 그러면 관리자가 HDFS 복제 요인을 줄일 수 있는 한편, 영역을 사용할 수 없는 사건이 발생할 때 데이터가 손실되지 않도록 할 수 있습니다.</li>
<li>이 시나리오에서는 <a href="http://hadoop.apache.org/docs/stable/cluster_setup.html#Hadoop+Rack+Awareness">HDFS 랙 인식</a> 기능을 사용하여 DataNode 그룹이 해제되는 동안 클러스터가 계속 실행되도록 합니다. HDFS NameNode가 블록 복제본을 별도의 랙에서 <a href="https://issues.apache.org/jira/secure/attachment/12345251/Rack_aware_HDFS_proposal.pdf">구성</a>한다는 사실을 잘 활용하면 해제 과정 동안 데이터가 여러 영역에서 복사되도록 할 수 있습니다. 랙은 같은 영역에서 실행되는 DataNode 모음으로 시뮬레이션되며, 영역당 정확히 두 개만 구성됩니다.</li>
</ol>
<figure><img src="/images/articles/managing-hadoop-clusters-on-gce/minimizing-unavailability.png" alt="">
<figcaption>그림 1: 영역 마이그레이션 과정 중 비가용성 최소화</figcaption>
</figure>
<h5>접근 방식</h5>
<ol>
<li><span class="bld">Commission new DataNodes and TaskTrackers.</span> New instances for these nodes can be created programmatically using the <span class="code">gcutil</span> utility, REST API or Google client libraries, or manually via the <a href="http://cloud.google.com/console">Google Cloud Console</a>. The nodes must then be added into the Hadoop cluster in one of the following ways:
<ol type="a">
<li>클러스터를 처음 가동할 때 노드의 호스트 이름이 <span class="code">slaves, dfs.hosts/mapred.hosts</span> 또는 <span class="code">dfs.include/mapred.include</span> 파일<sup><a href="#ftnt5" name="ftnt_ref5" id="ftnt_ref5">[5]</a></sup>에 존재하는지 확인하여 이러한 노드가 클러스터에 자동으로 연결되도록 합니다. 이 접근 방식에서는 이러한 이름이 사용 전에 알려져 있어야 하며, 사용자가 제공한 호스트 이름<sup><a href="#ftnt6" name="ftnt_ref6" id="ftnt_ref6">[6]</a></sup>으로 Google Compute Engine 인스턴스를 처리할 수 있기 때문에 이 방식이 가능한 것입니다.</li>
<li>가동이 시작되면 새 인스턴스 이름을 수동으로 이러한 파일에 추가한 다음, <span style="white-space:nowrap;" class="code">hadoop dfsadmin -refreshNodes</span>를 실행하여 DataNodes 및 <span style="white-space:nowrap;" class="code">hadoop mradmin -refreshNodes</span>를 요청하도록 NameNode에 알리고 TaskTracker에 대한 정보를 JobTracker에 알립니다.</li>
</ol>
새 DataNode는 소스 영역의 식별자와 다른 새로운 랙 식별자 쌍으로 자신을 식별합니다. 이런 방식으로 DataNode를 HDFS 랙으로 구성하면 랙이 해제되었을 때 모든 데이터 블록의 사본이 영역에서 최대 한 개만 제거되도록 설정할 수 있습니다. 또한 HDFS 복제를 사용하면 모든 데이터 블록의 사본이 적어도 두 개의 서로 다른 랙에 존재하게 됩니다. 따라서 이 과정을 통해 해제된 랙에 있는 모든 데이터 블록의 사본이 도착 영역의 랙에서 적어도 한 개는 생성됩니다.</li>
<li><span class="bld">NameNode 및 JobTracker를 마이그레이션합니다.</span> 이 단계를 구현하는 몇 가지 방법이 있습니다. 자동 방식을 사용하면 과정을 크게 간소화할 수 있습니다. 맞춤 구성을 수행할 때는 수동 방식이 필요할 수도 있습니다.
<p class="ital">자동</p>
<ol type="a">
<li>클러스터가 대기 상태인지 확인하고 마스터 서비스를 중단합니다.</li>
<li><a class="code" href="https://developers.google.com/compute/docs/gcutil/tips#moving">gcutil moveinstances</a> 명령을 사용하여 노드를 도착 영역으로 마이그레이션합니다. 이 명령은 인스턴스 구성을 복사하고, 디스크 스냅샷을 캡처하고, 기존 인스턴스를 삭제한 다음 도착 영역에 인스턴스를 다시 생성합니다.</li>
<li>새 인스턴스가 준비되고 서비스가 실행되면 클러스터에서 작업을 수락할 수 있습니다.</li>
</ol>
<p class="ital">수동</p>
<ol type="a">
<li>클러스터가 대기 상태인지 확인하고 마스터 서비스를 중단합니다.</li>
<li>나중에 사용할 수 있도록 루트 및 데이터 디스크의 스냅샷을 생성합니다.</li>
<li>소스 영역의 노드를 종료합니다.</li>
<li>이전에 생성한 스냅샷에서 새 루트 및 데이터 디스크를 생성합니다.</li>
<li>이전 단계에서 생성한 새 루트 및 데이터 디스크를 사용하여 도착 영역에 노드의 인스턴스를 생성합니다.</li>
<li>새 노드에서 마스터 서비스를 시작합니다.</li>
<li>새 인스턴스가 준비되고 서비스가 실행되면 클러스터에서 작업을 수락할 수 있습니다.</li>
</ol>
</li>
<li><span class="bld">DataNode 및 TaskTracker의 랙 한 개를 해제합니다.</span> 소스 영역의 랙 중 하나에 할당된 노드를 해제하려면 다음 단계를 수행합니다.
<ol type="a">
<li>이전에 1단계에서 나와 있는 파일에서 호스트 이름을 제거하고, 이 호스트 이름을 <span class="code">dfs.hosts.exclude</span> 및 <span class="code">mapred.hosts.exclude</span> 파일에 추가합니다.</li>
<li><span class="code">hadoop dfsadmin -refreshNodes</span> 및 <span class="code">hadoop mradmin -refreshNodes</span>를 실행합니다. 이 순서대로 진행하면 데이터 블록이 도착 영역으로 복제되기 시작합니다. DataNode 해제 과정은 즉시 이루어지지 않으며<sup><a href="#ftnt7" name="ftnt_ref7" id="ftnt_ref7">[7]</a></sup>, NameNode의 웹 인터페이스 등을 통해 각 노드의 처리 상태를 모니터링해야 합니다.</li>
<li>노드에서 'Decommissioned'(해제됨) 상태를 보고하면 노드를 안전하게 종료할 수 있습니다.<br></li>
</ol>
</li>
<li><span class="bld">DataNode 및 TaskTracker의 다른 랙을 해제합니다.</span> <span class="code">랙 1</span>의 노드가 모두 해제되면 도착 영역의 DataNode에 각 블록의 사본이 적어도 한 개는 존재하게 됩니다. 이 시점에서 이전 단계와 마찬가지로 소스 영역의 <span class="code">랙 2</span>에 있는 모든 인스턴스를 해제할 수 있습니다.</li>
</ol>
<h4>시나리오 2: 관리 간소화</h4>
<p>이 시나리오에서는 하나의 명령으로 마이그레이션을 수행하는 방법을 제시합니다. 이 방법에서는 클러스터 다운타임이 늘어날 수 있지만, 마이그레이션 작업을 쉽게 수행할 수 있습니다. 이 과정은 이전 시나리오에서 설명한 NameNode 및 JobTracker 인스턴스를 자동으로 마이그레이션하는 과정과 거의 동일합니다. <span class="code">gcutil moveinstances</span>에서 영역 간에 전체 클러스터를 마이그레이션합니다.</p>
<figure><img src="/images/articles/managing-hadoop-clusters-on-gce/simplifying-management.png" alt="">
<figcaption>그림 2: 영역 마이그레이션 관리 간소화</figcaption>
</figure>
<h5>접근 방식</h5>
<ol>
<li><span class="bld">클러스터를 마이그레이션합니다.</span> <span class="code">gcutil moveinstances</span> 명령을 사용하여 클러스터의 모든 노드와 관련 디스크를 도착 영역에 복제합니다. 예를 들어 단일 마스터 노드(<span class="code">hadoop-hm</span>)와 10개의 작업자 노드(<span class="code">hadoop-hw-000, …, hadoop-hw-009</span>)가 있는 소규모 클러스터를 생각해 볼 수 있습니다. 다음 단일 호출은 <span class="code">us-central2-a</span> 영역에서 클러스터를 이동한 후 <span class="code">us-central1-a</span> 영역에서 클러스터를 다시 시작합니다.
<p class="code">gcutil --project=&lt;project&gt; moveinstances hadoop-hm hadoop-hw-00\\d+ <span style="white-space:nowrap;">--source_zone=us-central2-a</span> --destination_zone=us-central1-a</p>
</li>
</ol>
<h3>재해 복구</h3>
<p>한 영역 전반에 이르는 심각한 장애는 극히 드물지만, 항상 예상치 못한 상황<sup><a href="#ftnt8" name="ftnt_ref8" id="ftnt_ref8">[8]</a></sup>을 대비해 두는 것이 좋습니다.</p>
<p>재해 복구를 처리하는 한 가지 방법은 디스크의 스냅샷을 주기적으로 생성하는 것입니다. 스냅샷은 지역에 따라 복제되며 Google Compute Engine 영역 외부에 존재합니다. 필요한 경우 스냅샷을 사용하여 복구 영역에서 대체 인스턴스를 위한 새 디스크를 생성할 수 있습니다.</p>
<h5>고려사항</h5>
<ol>
<li>스냅샷이 존재하는 영역에서 중단이 발생해도 디스크는 손실되지 않지만, 영역이 복원될 때까지는 디스크를 사용할 수 없습니다. 스냅샷은 클러스터 재구성을 빠르게 할 수 있는 기술입니다.</li>
</ol>
<figure><img src="/images/articles/managing-hadoop-clusters-on-gce/persistent-disks.png" alt="">
<figcaption>그림 3: 재해 복구를 위한 영구 디스크</figcaption>
</figure>
<h5>접근 방식</h5>
<ol>
<li><span class="bld">스냅샷을 자주 캡처합니다</span>. 영구 디스크의 정기적인 스냅샷<sup><a href="#ftnt9" name="ftnt_ref9" id="ftnt_ref9">[9]</a></sup>을 생성하기 위한 메커니즘을 만듭니다. 데이터 일관성이 저하되는 가능성을 줄이기 위해 스냅샷 생성 시 클러스터를 대기 상태로 두는 것이 좋습니다.</li>
<li><span class="bld">스냅샷에서 디스크를 다시 생성합니다</span>. 클러스터를 호스팅하는 영역을 사용할 수 없게 되는 경우 스냅샷에서 새 영구 디스크를 생성할 수 있습니다.</li>
<li><span class="bld">대체 노드를 가동합니다</span>. 새 영구 디스크를 사용하여 클러스터에 있는 모든 노드의 새 인스턴스를 새 영역에서 가동할 수 있습니다.</li>
<li><span class="bld">(선택사항) HDFS를 다시 채웁니다</span>. DataNode에서 영구 디스크를 사용하거나 스냅샷을 캡처하지 않은 경우 새 HDFS를 데이터로 다시 채워야 합니다.</li>
</ol>
<a href="#" name="id.db8xgq3x77ts" id="id.db8xgq3x77ts"></a>
<h4>다중 영역 클러스터</h4>
<p>또 다른 방법<sup><a href="#ftnt10" name="ftnt_ref10" id="ftnt_ref10">[10]</a></sup>은 여러 영역을 아우르는 클러스터를 배포하는 것입니다. 더 구체적으로는, 두 번째 영역에서 DataNode 및 TaskTracker의 추가적인 집합을 요청하고, 적용 가능한 경우에는 NameNode 또는 JobTracker 대기 노드를 배치합니다.</p>
<h5>고려사항</h5>
<ol>
<li>이 패턴에서는 다른 방식에서 배포되는 DataNode 및 TaskTracker 수의 두 배가 필요하며, 상당한 수준의 영역 간 통신이 필요합니다. 이 패턴을 사용하면 처리 능력이 두 배가 되지만 운영(인스턴스, 저장소 및 네트워크) 비용이 증가하고 전반적인 성능이 저하될 수 있습니다. 특히 여러 영역에서 데이트가 액세스되는 경우에 더욱 그렇습니다.</li>
<li>일부 배포판에서는 NameNode 및 JobTracker에 고가용성 기능을 제공합니다. 관련 배포에 대한 자세한 내용은 문서 후반부의 고가용성 섹션을 참조하시기 바랍니다.</li>
<li>rsync, NFC 또는 기타 분산 파일 시스템과 같은 다른 기술을 사용하여 NameNode 디렉토리<sup><a href="#ftnt11" name="ftnt_ref11" id="ftnt_ref11">[11]</a></sup>를 동기화할 수 있습니다.</li>
</ol>
<figure><img src="/images/articles/managing-hadoop-clusters-on-gce/multi-zone-cluster.png" alt="">
<figcaption>그림 4: 재해 복구를 위한 다중 영역 클러스터</figcaption>
</figure>
<h5>접근 방식</h5>
<ol>
<li><span class="bld">클러스터를 분배합니다</span>. DataNode 및 TastTracker가 두 영역 모두에서 동일하게 분배되며 완전하게 요청됩니다. 영역 내 DataNode는 동일한 랙으로 식별되어야 하며, 각 영역은 서로 다른 랙 식별자를 사용해야 합니다. 그래야 각 데이터 블록의 사본이 두 번째 영역에 적어도 한 개 존재할 수 있습니다.</li>
<li><span class="bld">NameNode 장애 조치를 수행합니다</span>. NameNode는 두 번째 영역에서 자동으로 복원하거나(이 기능을 제공하는 배포판인 경우), 대체 인스턴스를 생성하고(인스턴스를 실행 중이지 않은 경우) HDFS 서비스를 시작하여 수동으로 복원할 수 있습니다.</li>
<li><span class="bld">JobTracker 장애 조치를 수행합니다</span>. The JobTracker는 두 번째 영역에서 자동으로 복원하거나(이 기능을 제공하는 배포판인 경우), 대체 인스턴스를 생성하고 서비스를 시작하여 수동으로 복원할 수 있습니다. 수동 시나리오에서는 JobTracker를 새 인스턴스로 대체해야 합니다. 인스턴스의 호스트 이름을 변경하려면 클러스터 구성을 업데이트하고 TaskTracker를 다시 시작해야 합니다. 따라서 다른 이름이 필요한 경우가 아니라면 새 인스턴스 생성 시 같은 호스트 이름을 다시 사용하기만 하면 됩니다.</li>
</ol>
<h3>고가용성</h3>
<p>일반적으로 Hadoop 클러스터의 고가용성이라고 하면, 활성 HDFS NameNode가 제거되는 경우(예: 예상치 못한 장애가 발생하는 경우) 대기 모드의 자동 활성화를 통해 이로부터 복구할 수 있는 기능으로 이해되고 있습니다. 몇몇 공급업체에서는 자신의 Hadoop 1.x 배포판에 이 기능을 추가 기능으로 제공하고 있습니다. Hadoop 2.x에서는 이를 처리하기 위해 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithNFS.html">NFS 공유 저장소</a> 및 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">쿼럼 기반 저널링</a>과 같은 몇 가지 메커니즘을 통합했습니다.<sup><a href="#ftnt12" name="ftnt_ref12" id="ftnt_ref12">[12]</a></sup></p>
<p>Hadoop 1x의 JobTracker 및 Hadoop 2.x의 YARN ResourceManager 모두 단일 장애 지점으로 간주되어야 합니다. 서비스가 충돌하는 경우, 서비스를 다시 시작할 때까지 MapReduce를 사용할 수 없습니다. 구체적인 결과는 배포판에 따라 다르지만, 대부분의 경우 계속 실행되는 작업은 재개되는 대신 다시 시작됩니다. 하지만 노드 자체가 충돌하는 경우, 수동으로 노드를 대체하거나 대기 모드에서 장애 조치를 지원하는 배포판을 사용해야 합니다.<sup><a href="#ftnt13" name="ftnt_ref13" id="ftnt_ref13">[13]</a></sup></p>
<p>Google Compute Engine에 고가용성 Hadoop을 설치하는 작업은 간단하지만 특정한 환경 관련 기능이 있으므로 더 주의해야 합니다.</p>
<h4>Google Compute Engine 네트워크 DNS</h4>
<p>Google Compute Engine의 이점 중 하나는 사용자가 제공한 호스트 이름으로 모든 지역의 모든 영역에 있는 네트워크의 인스턴스를 처리할 수 있는 기능을 갖추고 있다는 점입니다. 또한 인스턴스를 재활용(한 인스턴스를 종료하고 동일한 이름으로 다른 인스턴스를 가동)하는 경우 이러한 이름을 다시 사용할 수도 있으며, 이는 이름 기반 처리에 의존하는 배포에서 편리하게 사용할 수 있습니다. 또한 내부 IP 주소 할당과 관련하여 보장되는 사항이 없다는 점도 기억해 두면 좋습니다. 인스턴스 종료 시 DNS 항목이 즉시 정리되므로 인스턴스 호스트 이름을 동적으로 재사용할 수 있습니다.</p>
<p>현재 H/A에 QJM(Quorum Journal Manager)을 사용하도록 구성한 Hadoop HDFS 2.0은 이러한 동작 때문에 장애에 <a href="https://issues.apache.org/jira/browse/HDFS-4957">취약</a>합니다. 특히 활성 NameNode 인스턴스가 종료되는 경우, 자동 장애 조치 과정에서 노드의 호스트 이름을 확인할 수 없으므로 충돌이 발생하게 됩니다. 한 가지 해결방법은 각 인스턴스의 <span class="code">/etc/hosts</span> 파일에 NameNode 호스트 이름 및 IP 주소를 추가하는 것입니다. 그러면 QJM이 표시되어 시간 제한을 올바르게 처리하므로 자동 장애 조치를 정상적으로 수행할 수 있습니다. 이 해결방법을 사용하는 경우 새 NameNode를 가동할 때마다 파일을 최신 상태로 유지해야 하므로 유지관리 문제가 어느 정도 발생할 수 있습니다.</p>
<h4>ZooKeeper</h4>
<p><a href="http://zookeeper.apache.org/">Apache ZooKeeper</a>는 HDFS 고가용성을 위한 자동 장애 조치 기능을 제공하기 위해 Hadoop 2.0에서 사용되며, NFS 및 Quorum Journal Manager를 모두를 사용합니다. Hadoop용 ZooKeeper를 Google Compute Engine에 배포하는 경우 DNS와 관련하여 특별한 고려사항을 확인하는 것이 좋습니다.</p>
<p>현재 ZooKeeper에서는 시작 시 DNS 이름을 <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-338">1회</a> 참조합니다. 그러면 인스턴스 그룹의 나머지 인스턴스를 재부팅하지 않고 한 구성 인스턴스가 대체되는 것을 방지할 수 있습니다. 따라서 Google Compute Engine DNS의 동작에 대한 이전 설명을 고려하면, ZooKeepr를 실행하는 노드가 다시 부팅되거나 대체될 때마다 다른 모든 ZooKeeper 서비스를 다시 시작해야 합니다. 그렇지 않으면 기존 인스턴스 그룹의 구성 인스턴스 하나가 비게 되며, 새 인스턴스는 그룹에 속하지 않은 ZooKeeper 인스턴스를 실행하게 됩니다.</p>
<h4>다중 영역 처리</h4>
<p>영역 장애에 대한 클러스터의 장애 복구성의 측면에서도 고가용성을 고려해 볼 수 있습니다. 이는 장기간 실행되는 작업 또는 파이프라인이 중단되는 경우 품질이 떨어지거나 이로 인해 상당한 비용이 발생할 수 있는 시나리오에 적용할 수도 있습니다. 영역 A가 <span class="code">영역 B</span>보다 먼저 유지관리가 예약되어 있는 시나리오를 생각해 보겠습니다. 설명된 내용에 따라, 두 영역 중 하나가 중단되는 경우에도 클러스터가 계속 실행될 수 있도록 두 영역을 아우르는 클러스터를 구축하는 것이 가능합니다.</p>
<figure><img src="/images/articles/managing-hadoop-clusters-on-gce/straddling-multiple-zones.png" alt="">
<figcaption>그림 5: 고가용성을 위해 여러 영역 포함</figcaption>
</figure>
<ol>
<li><span class="bld">클러스터를 분배합니다</span>. 이전에 설명한 것처럼, DataNode 및 TaskTracker는 두 영역에서 동일하게 분배되며 HDFS 클러스터의 일부로 요청됩니다. 또한 데이터가 완전히 복제되도록 여러 HDFS 랙이 사용됩니다.</li>
<li><span class="bld">ZooKeeper 인스턴스 그룹을 분배합니다</span>. 대부분의 ZooKeeper 노드는 <span class="code">대기</span> NameNode 및 <span class="ital">활성</span> JobTracker와 함께 <span class="ital">영역 B</span>에 배포해야 합니다.</li>
<li><span class="bld">NameNode를 관리합니다</span>. <span class="code">영역 A</span>를 사용할 수 없게 되는 경우 ZooKeeper 인스턴스 그룹은 계속해서 쿼럼을 유지하고, 자동으로 장애 조치와 대기 NameNode의 활성화 전환을 수행할 수 있도록 합니다. 영역 A가 아닌 <span class="code">영역 B</span>가 중단되는 경우 활성 NameNode가 계속해서 실행됩니다. 하지만 인스턴스 그룹에 쿼럼이 없으므로 클러스터에서 활성 노드 장애에 대처할 수 없게 됩니다. 따라서 대체 ZooKeeper 노드를 추가하고 인스턴스 그룹을 다시 시작해야 합니다.<sup><a href="#ftnt14" name="ftnt_ref14" id="ftnt_ref14">[14]</a></sup></li>
<li><span class="bld">JobTracker를 관리합니다</span>. 활성 JobTracker는 <span class="code">영역 B</span>에 배포됩니다. 따라서 <span class="code">영역 A</span>가 (예상 시나리오대로) 사용할 수 없게 되는 경우 모든 활성 작업이 완료될 때까지 계속 실행됩니다. 하지만 배포판이 JobTracker 장애 조치를 수행하는 데 ZooKeeper에 의존하거나 배포판에서 이 기능을 지원하지 않는 경우, 대체 조치를 수행해야 하며 실행되고 있는 모든 작업을 다시 제출해야 할 가능성이 높습니다. <a href="#id.db8xgq3x77ts">다중 영역 클러스터</a> 재해 복구 시나리오에 설명되어 있는 옵션 중 하나로 이러한 배포를 확장할 수 있습니다.</li>
</ol>
<h2>맺음말</h2>
<p>Google Cloud Platform은 Hadoop 실행을 위한 고성능 플랫폼을 제공할 뿐만 아니라, 여러 영역 사이에서 클러스터를 유지관리하는 데 도움이 되는 기능과 도구를 제공합니다. 따라서 영역 마이그레이션, 재해 복구 및 고가용성 시나리오와 같은 상황에서 비즈니스에 중요한 작업을 계속해서 수행할 수 있습니다. 영구 디스크, Google Cloud Storage 및 네트워크 인프라를 사용하면 데이터와 인스턴스를 효율적으로 마이그레이션할 수 있습니다. 또한 <span class="code">gcutil</span>은 클러스터 관리 작업을 수행하는 데 도움이 되는 풍부한 명령 집합을 제공합니다.</p>
<h2>추가 리소스</h2>
<ul>
<li><a href="/resources/articles/apache-hadoop-hive-and-pig-on-google-compute-engine">Google Compute Engine의 Apache Hadoop, Hive 및 Pig</a> Google Cloud Platform 솔루션 – Google Cloud Storage의 지원 기능과 함께 Google Compute Engine을 활용하여 Apache Hive 및 Apache Pig가 포함된 자체 관리되는 Hadoop 클러스터를 빅 데이터 처리 솔루션의 일부로 실행하는 방법에 대해 자세히 알아보세요.</li>
<li><a href="https://github.com/GoogleCloudPlatform/solutions-google-compute-engine-cluster-for-hadoop">google-compute-engine-cluster-for-hadoop</a>: Hadoop 컴퓨팅 클러스터를 설정하고 MapReduce 작업을 실행하는 데 도움을 주는 샘플 애플리케이션입니다. 이 애플리케이션은 이 문서에서 설명한 클러스터 관리를 전혀 수행하지 않습니다.</li>
</ul>
<br>
<hr>
<p><a href="#ftnt_ref1" name="ftnt1" id="ftnt1">[1]</a> Google Compute Engine의 빠른 가상 머신 생성 기능과 분 단위 결제 모델을 통해 고객은 즉석 클러스터를 사용하여 데이터를 처리할 수 있으며, 고정된 가격 책정으로 제한된 리소스 관리에서 발생하는 추가 비용을 지불하지 않아도 됩니다.</p>
<p><a href="#ftnt_ref2" name="ftnt2" id="ftnt2">[2]</a> 이러한 제한을 완화하면 Google Cloud Storage를 지속성 있는 저장소로 활용하는 데 선호하는 방식을 사용할 수 있습니다.</p>
<p><a href="#ftnt_ref3" name="ftnt3" id="ftnt3">[3]</a> Apache Hadoop 1.1.2와 같이 고가용성 기능을 제공하지 않는 배포를 말합니다.</p>
<p><a href="#ftnt_ref4" name="ftnt4" id="ftnt4">[4]</a> 2013년 7월 현재, 표준 인스턴스 유형에 최대 10TB의 영구 디스크 저장소를 마운트할 수 있습니다.</p>
<p><a href="#ftnt_ref5" name="ftnt5" id="ftnt5">[5]</a> 대부분의 배포에서 보통 이러한 파일을 무시하지만, 클러스터에서 이러한 노드를 원활하게 요청하고 해제하는 것을 지원하기 위해 파일이 필요합니다. 자세한 내용은 Hadoop <a href="http://hadoop.apache.org/docs/stable/cluster_setup.html">클러스터 설정</a> 및 <a href="http://wiki.apache.org/hadoop/FAQ">FAQ</a>를 참조하세요.</p>
<p><a href="#ftnt_ref6" name="ftnt6" id="ftnt6">[6]</a> Google Compute Engine 인스턴스 이름은 내부 IP 주소로 지정되지 않으며, 추가적인 이름 확인자를 사용하지 않고도 생성 시 제공된 호스트 이름으로 처리할 수 있습니다.</p>
<p><a href="#ftnt_ref7" name="ftnt7" id="ftnt7">[7]</a> 노드 해제에 대한 자세한 내용은 <a href="http://wiki.apache.org/hadoop/FAQ">Hadoop FAQ</a> §3.6을 참조하세요.</p>
<p><a href="#ftnt_ref8" name="ftnt8" id="ftnt8">[8]</a> <a href="https://developers.google.com/compute/docs/sla">Google Compute Engine 서비스 수준 계약</a></p>
<p><a href="#ftnt_ref9" name="ftnt9" id="ftnt9">[9]</a> 스냅샷 생성에 대한 자세한 내용은 <a href="https://developers.google.com/compute/docs/disks#snapshots">이 문서</a>를 참조하세요.</p>
<p><a href="#ftnt_ref10" name="ftnt10" id="ftnt10">[10]</a> 이 옵션은 완벽을 기하기 위한 용도로 제공되며, 추가 비용이 발생하고 성능에 영향을 줄 수 있으므로 보통 권장하지 않습니다.</p>
<p><a href="#ftnt_ref11" name="ftnt11" id="ftnt11">[11]</a> 이러한 메커니즘의 설계 및 구현은 이 문서의 범위에 포함되지 않으며, 이를 제작 상황에서 사용하려면 항상 엄격한 테스트를 거쳐야 합니다.</p>
<p><a href="#ftnt_ref12" name="ftnt12" id="ftnt12">[12]</a> 2013년 7월 현재, Apache Hadoop 2.0은 아직 알파 상태입니다.</p>
<p><a href="#ftnt_ref13" name="ftnt13" id="ftnt13">[13]</a> 2013년 7월 현재, Apache Hadoop에서는 JobTracker 또는 YARN ResourceManager를 위한 고가용성 기능을 지원하지 않지만, YARN ResourceManager를 위한 지원은 <a href="https://issues.apache.org/jira/browse/YARN-149">제안</a>된 상태입니다.</p>
<p><a href="#ftnt_ref14" name="ftnt14" id="ftnt14">[14]</a> 2013년 7월 현재, ZooKeeper 3.5에서 <a href="http://issues.apache.org/jira/browse/ZOOKEEPER-107">동적 멤버십</a>에 대한 지원을 추가했지만, Hadoop 2.0.5-알파는 ZooKeeper 3.4.2를 사용합니다.</p>
</div>
<!-- /maia-main --></div>
<div class="maia-col-3"><br>
<img src="//www.google.com/images/icons/product/feedback-16.png" class="g-app-icon" alt=""> <a href="javascript:void(0);" class="google-feedback">이 문서에 대한 의견 보내기</a><br>
<br></div>
</div>
</div>
<script>
(function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><script src="//www.gstatic.com/feedback/api.js">
</script><script>
$('body').delegate('.google-feedback', 'click', function() {
      userfeedback.api.startFeedback({'productId': '94614'});
  });
</script><!-- Scripts to include both on Goro + Devsite --><script>
window.___gcfg = {
     lang: ''
   };
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><!-- GTM implementation --><!-- Start dataLayer --><script>
dataLayer = [{
        'country': 'ko',
        'region': 'japac',
        'language': 'ko'
      }];
</script><!-- End dataLayer --><!-- Start Google Tag Manager --><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5CVQBG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5CVQBG');
</script><!-- End Google Tag Manager --><!-- Global JS scripts to load; path will depend on whether we're on devsite or Goro --><script src="/js/base.min.js">
</script><!-- Retina loader; Do not load if partners page because we need to wait until the Angular app runs --><script>
new lfl.system.RetinaLoader();
</script><!-- Secondary right-side scroll-nav --><script>
new lfl.ui.ScrollNav({});
</script>
</body>
</html>