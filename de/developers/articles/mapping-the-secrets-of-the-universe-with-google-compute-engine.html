<!DOCTYPE html>
<html devsite="">
<head>
<title>Big Data Articles &amp; Tutorials — Google Cloud Platform</title>
<meta name="description" content="Read about technical big data articles and solutions with Google Cloud Platform. Big data articles unpack BigQuery data analytics, Hadoop clusters and more.">
<meta name="hide_page_heading" value="true">
<meta name="full_width" value="true">
<meta name="top_category" value="developers">
<meta name="subcategory" value="articles">
<meta name="viewport" content="initial-scale=1, minimum-scale=1, width=device-width">
<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link href="//fonts.googleapis.com/css?family=Open+Sans:400italic,300,400,600,700" rel="stylesheet">
<link href="/css/default.css" rel="stylesheet"><!--[if lt IE 9]>
    <link rel="stylesheet" media="screen" href='/css/cp-ie.css'>
    <![endif]-->
<script src="/js/floodlight.js">
</script>
</head>
<body>
<div id="maia-main" class="cp-article">
<div class="maia-cols">
<div class="maia-col-9">
<div>
<div style="float:right">
<div class="g-plusone"></div>
</div>
<h1 class="title">Geheimnisse des Universums mit Google Compute Engine abbilden</h1>
</div>
<div class="cp-article-tutorial">
<h2>Organisation</h2>
<p>ATLAS ist ein Experiment der Teilchenphysik am Large Hadron Collider bei CERN in Genf, Schweiz. Der ATLAS-Detektor untersucht die Kollisionen von Protonen und Schwerionen mit außergewöhnlich hoher Energie, um neue Erkenntnisse zu gewinnen. <a href="http://en.wikipedia.org/wiki/ATLAS_experiment">ATLAS</a> nutzt ein Grid-Computing-Modell zur Organisation der verteilten Ressourcen. Im Rahmen des ATLAS-Experiments werden fast 200&nbsp;Standorte für mehr als 3000&nbsp;Forscher weltweit verwaltet. Vor Kurzem nutzte ATLAS Google Compute Engine zur Erweiterung der Kapazitäten und zur Befriedigung des Bedarfs für ständig zunehmende Lasten. Dabei zeigte sich, wie einfach eine nahtlose Integration von Google Compute Engine in das ATLAS-Computer-Grid möglich ist. Außerdem ist das Projekt ein Beleg dafür, dass physikalische Forschung in der Cloud durchgeführt werden kann.</p>
<h2>Herausforderung</h2>
<p>Sämtliche verfügbare Ressourcen des ATLAS-Computer-Grids werden das ganze Jahr über voll beansprucht. Laut ATLAS können die Nutzer- und Gruppenanalyse von Beobachtungsdaten sowie die Operationen für Ereignisgeneratoren und Detektorsimulationen zu mehr als einer Million Arbeitsaufträge pro Tag führen. Allerdings geht man davon aus, dass der Bedarf diesen Wert häufig um das Doppelte übertrifft und zu bestimmten Zeiten während des Jahres die Kapazität des Grids von mehr als 100.000&nbsp;CPU-Kernen bei weitem überschreitet. Ein solcher Rückstand kann einen Planungs- und Priorisierungsdruck für das System bedeuten, der die Forschungsarbeiten ausbremst. Im Wesentlichen verlangsamt der Mangel an Rechenressourcen so den wissenschaftlichen Fortschritt. Aus diesem Grund richtete ATLAS ein FE-Projekt zur Untersuchung von Virtualisierungsoptionen und zur Abklärung ein, ob die Nachfragespitzen für Rechenressourcen mithilfe von Cloudcomputing bewältigt werden können.</p>
<h2>Lösung</h2>
<p>ATLAS begann im Herbst&nbsp;2012 mit der Verwendung von Google Compute Engine. Die erste Maßnahme zielte auf die Erstellung leistungsstarker Analysecluster ab wie diejenigen, die von ATLAS für die interaktive und stapelweise Analyse großer Datensätze aus Experimenten der Teilchenphysik verwendet werden. "Der Zugriff auf Google Compute Engine ermöglichte es uns, große <a href="http://root.cern.ch/drupal/content/proof">PROOF</a>-Cluster für bis zu 1.000&nbsp;Worker zu erstellen und zu testen", sagte Dr. Sergey Panitkin vom Brookhaven National Lab, Leiter der F&amp;E-Abteilung Cloud-Computing für ATLAS. Diese parallelen Datenverarbeitungscluster reagieren sensibel auf die Leistung der zugrunde liegenden Speicherinfrastruktur. Memory-only-Konfigurationen sowie Setups mit Scratch Disk und persistenten Festplatten wurden Benchmark-Tests unterzogen.</p>
<p>Der Schwerpunkt der zweiten Maßnahme bestand in einer Kapazitätserweiterung in Form von 4.000&nbsp;Kernen, die als eine <a href="http://research.cs.wisc.edu/htcondor/">HTCondor</a>-basierte <a href="http://iopscience.iop.org/1742-6596/219/6/062028">PanDA</a>-Warteschlange organisiert und transparent in das ATLAS-Computer-Grid einbezogen wurden. Dieser Cluster wurde erstellt, um CPU-intensive Monte-Carlo-Simulationslasten auszuführen. Das Ziel bestand im Testen der langfristigen Stabilität eines cloudbasierten Clusters, dessen Größe der eines großen ATLAS (Tier-2)-Grid-Standorts ähnelte. Eine wichtige Anforderung war die Umsetzung in Form einer vollständigen Produktionsplattform, nicht nur als FE-Machbarkeitsnachweis.</p>
<h2>Ergebnisse</h2>
<p>ATLAS berichtete, dass die Skalierbarkeit und Leistungsfähigkeit der Google Compute Engine-Infrastruktur die Abwicklung E/A-intensiver Lasten auf beispiellosen Niveaus ermöglichte. Es war das erste Mal, dass Tests dieser Größenordnung durchgeführt werden konnten. Mithilfe eines 500&nbsp;Worker umfassenden PROOF-Analyseclusters, der ausschließlich in Google Compute Engine ausgeführt wurde und einen Durchsatz von 210&nbsp;Millionen Zyklen pro Sekunde bewältigen konnte, ermittelte das Team Optimierungspotenzial, das bei ähnlichen Tests in der Infrastruktur der eigenen Einrichtungen nicht ersichtlich war.</p>
<p>Darüber hinaus wurde der Cluster fast zwei Monate lang als Teil des verteilten ATLAS-Computer-Grids betrieben, was 5&nbsp;Millionen Kernstunden, die Abarbeitung von 458.000&nbsp;rechenintensiven Aufträgen und die Verarbeitung von 214&nbsp;Millionen Ereignissen umfasste. Der Cluster erzielte einen nachhaltigen Spitzendurchsatz von 15.000&nbsp;Aufträgen pro Tag. "Wir haben mit Google Compute Engine großartige Erfahrungen gemacht ... und wir sind der Ansicht, dass die moderne Cloudinfrastruktur als eine stabile, leistungsstarke Plattform für wissenschaftliches Rechnen dienen kann", schlussfolgerte Dr. Panitkin.</p>
</div>
<!-- /maia-main --></div>
<div class="maia-col-3"><br>
<img src="//www.google.com/images/icons/product/feedback-16.png" class="g-app-icon" alt=""> <a href="javascript:void(0);" class="google-feedback">Feedback zu diesem Dokument</a><br>
<br>
<!-- <img src="/images/articles/mapping-the-secrets/ATLAS-chrome-logo-blue-wh_lo.png" alt="Atlas Experiment"> -->
<p class="qte">"Das Projekt war auf der Cloudseite sehr stabil – Google Compute Engine war grundsolide ... es gab keine Ausfälle aufgrund von GCE-Problemen."</p>
<p class="qte-author">– Dr. Sergey Panitkin, Brookhaven National Lab</p>
</div>
</div>
</div>
<script>
(function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><script src="//www.gstatic.com/feedback/api.js">
</script><script>
$('body').delegate('.google-feedback', 'click', function() {
      userfeedback.api.startFeedback({'productId': '94614'});
  });
</script><!-- Scripts to include both on Goro + Devsite --><script>
window.___gcfg = {
     lang: ''
   };
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><!-- GTM implementation --><!-- Start dataLayer --><script>
dataLayer = [{
        'country': 'de',
        'region': 'emea',
        'language': 'de'
      }];
</script><!-- End dataLayer --><!-- Start Google Tag Manager --><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5CVQBG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5CVQBG');
</script><!-- End Google Tag Manager --><!-- Global JS scripts to load; path will depend on whether we're on devsite or Goro --><script src="/js/base.min.js">
</script><!-- Retina loader; Do not load if partners page because we need to wait until the Angular app runs --><script>
new lfl.system.RetinaLoader();
</script><!-- Secondary right-side scroll-nav --><script>
new lfl.ui.ScrollNav({});
</script>
</body>
</html>