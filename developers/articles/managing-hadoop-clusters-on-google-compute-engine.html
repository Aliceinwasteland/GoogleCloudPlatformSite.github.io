---
title: Managing Hadoop Clusters on Google Compute Engine
---
<div id="maia-main" class="cp-article">
  <div class="maia-cols">
    <div class="maia-col-9">
      <div>
        <div style="float:right">
          <div class="g-plusone"></div>
        </div>
        <h1 class="title">Managing Hadoop Clusters on Google Compute Engine</h1>
      </div>
      
<div class="cp-article-tutorial">

  <h2>Introduction</h2>
  <p>Google Compute Engine is a <a href="http://www.mapr.com/press-release/mapr-and-google-compute-engine-set-new-world-record-for-hadoop-terasort">proven</a> platform for running Hadoop. Consistent, high performance virtual machines coupled with a high bandwidth, low latency network linking them together and to the rest of the Google Cloud Platform services deliver a solid foundation for cloud-based data processing architectures. The ability to quickly instantiate numerous virtual machines on demand and the availability of per-minute pricing make Compute Engine well-suited and cost-effective for spinning up and turning down ad hoc clusters. Further, Compute Engine&rsquo;s advanced routing and data encryption features together with Google&rsquo;s global network performance enable the construction of secure, compelling hybrid solutions.</p>
  <p>This paper presents several techniques for those who wish to manage their own Hadoop installations on Google Compute Engine, and select scenarios (migration across zones, disaster recovery and high availability) that arise when dealing with long-lived clusters and operating across multiple zones. This list is neither exhaustive nor authoritative as other solutions certainly exist and might even be more applicable to specific situations. This paper, however, intends to show how some of the same features that make Compute Engine a powerful platform upon which to run Hadoop, when combined with the <a href="/compute/docs/gcutil/"><span class="code">gcutil</span></a> command line utility, can provide the basis for Hadoop cluster management solutions.</p>

  <h2>Scenarios</h2>
  <p>The scenarios presented in this section are all based on the premise that the cluster under consideration ought to be preserved to the greatest extent possible when faced with an adverse environmental event (for example, a zone experiences an unanticipated outage). This is an unlikely but possible situation for long-lived clusters used in support of job pipelines and stream processing. On-demand clusters<sup><a href="#ftnt1" name="ftnt_ref1" id="ftnt_ref1">[1]</a></sup> require little or no management and are, generally speaking, outside the scope of this document. Further, the scenarios presented assume that data consumed and generated by the cluster needs to reside durably in self-managed HDFS (Hadoop Distributed File System).<sup><a href="#ftnt2" name="ftnt_ref2" id="ftnt_ref2">[2]</a></sup></p>
  <p>Administrators can plan accordingly for maintenance events and manage cluster availability. However, should an unexpected situation occur, jobs currently being processed will likely be impacted. While specific outcomes vary by Hadoop <a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support">distribution</a>, in most cases, these jobs are restarted rather than resumed, though there is a possibility that jobs will need to be resubmitted.</p>

  <h3>Zone migration</h3>
  <p>At some point, it may become desirable or <a href="/compute/docs/faq#maintenance">necessary</a> to move or clone a cluster from one zone to another. This section presents two different scenarios for accomplishing such a task. The first scenario aims to minimize the unavailability of the cluster to accept and run MapReduce jobs, at the expense of operational complexity. The second scenario trades additional unavailability in favor of significantly simpler management.</p>

  <h4>Scenario 1: Minimizing unavailability</h4>
  <p>Many times, a cluster will be in steady use (particularly if only one exists), and any period during which users are unable to submit work can be costly. This first approach attempts to reduce this duration as much as possible in a traditional deployment.<sup><a href="#ftnt3" name="ftnt_ref3" id="ftnt_ref3">[3]</a></sup> First, new <a href="http://wiki.apache.org/hadoop/DataNode">DataNodes</a> and <a href="http://wiki.apache.org/hadoop/TaskTracker">TaskTrackers</a> (or <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN NodeManagers</a>) are added to the cluster in the destination zone, and then the master nodes (such as the <a href="http://wiki.apache.org/hadoop/NameNode">NameNode</a>, <a href="http://wiki.apache.org/hadoop/JobTracker">JobTracker</a>, or <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN ResourceManager</a>) are switched over as well. This method bounds the cluster&rsquo;s unavailability to roughly the interval defined from the shutdown of the master services in the source zone to their startup in the destination zone.</p>

  <h5>Considerations</h5>
  <ol>
    <li>DataNodes use <a href="/compute/docs/disks">persistent disks</a> for HDFS. Persistent disks offer durability beyond the lifetime of an instance. This affords administrators the option of reducing the HDFS replication factor while ensuring that no data will be lost in the event of a zone becoming unavailable.</li>

    <li>This scenario makes use of <a href="http://hadoop.apache.org/docs/stable/cluster_setup.html#Hadoop+Rack+Awareness">HDFS rack awareness</a> to enable the cluster to continue running while groups of DataNodes are decommissioned. The fact that the HDFS NameNode <a href="https://issues.apache.org/jira/secure/attachment/12345251/Rack_aware_HDFS_proposal.pdf">organizes</a> block replicas on separate racks can be used to ensure that data is copied across zones during this process. A rack is simulated as a collection of DataNodes running in the same zone, and exactly two per zone are configured.</li>
  </ol>

  <figure>
    <img src="/images/articles/managing-hadoop-clusters-on-gce/minimizing-unavailability.png">
    <figcaption>Figure 1: Minimizing unavailability during zone migration</figcaption>
  </figure>


  <h5>Approach</h5>
  <ol>
    <li>
      Commission new DataNodes and TaskTrackers. New instances for these nodes can be created programmatically using the <span class="code">gcutil</span> utility, REST API or Google client libraries, or manually via the <a href="https://console.developers.google.com">Google Cloud Console</a>. The nodes must then be added into the Hadoop cluster in one of the following ways:
      <ol type="a">
        <li>Enable these nodes to automatically join the cluster by ensuring that their hostnames exist in the <span class="code">slaves, dfs.hosts/mapred.hosts</span> or <span class="code">dfs.include/mapred.include</span> files<sup><a href="#ftnt5" name="ftnt_ref5" id="ftnt_ref5">[5]</a></sup> at the time the cluster is first brought up. This approach implies that these names must be known in advance of their use and is possible because Google Compute Engine instances are addressable by the user-provided hostname<sup><a href="#ftnt6" name="ftnt_ref6" id="ftnt_ref6">[6]</a></sup>. </li>
        <li>Manually add the new instance names to these files once they spin-up and then run <span style="white-space:nowrap;" class="code">hadoop dfsadmin -refreshNodes</span> to notify the NameNode to commission the DataNodes and <span style="white-space:nowrap;" class="code">hadoop mradmin -refreshNodes</span> to notify the JobTracker about the TaskTrackers.</li>
      </ol>
      The new DataNodes should identify themselves with a pair of new rack identifiers, different from those in the source zone. By organizing the DataNodes as HDFS racks in this manner, it can be established that when a rack is decommissioned, only one copy at most, of any data block, will be removed from the zone. Further, HDFS replication will ensure that a copy of every data block will exist on at least two different racks. Therefore, this process ensures that at least one copy of every data block in the decommissioned rack will be made to a rack in the destination zone.
    </li>
    <li>
      <span class="bld">Migrate NameNode and JobTracker.</span> There are several ways to implement this step. The automatic approach greatly simplifies the process. The manual approach may be necessary when performing custom configurations.

      <p class="ital">Automatically</p>
      <ol type="a">
        <li>Ensure the cluster is quiescent and shut down the master services.</li>
        <li>Use the <a class="code" href="/compute/docs/gcutil/tips#moving">gcutil moveinstances</a> command to migrate the nodes into the destination zone.  This command copies instance configurations, takes snapshots of the disks, deletes the existing instances, and then recreates the instances in the destination zone.</li>
        <li>Once the new instances are up and services are running, the cluster is available to accept jobs.</li>
      </ol>
      <p class="ital">Manually</p>
      <ol type="a">
        <li>Ensure the cluster is quiescent and shutdown the master services.</li>
        <li>Create snapshots of root and data disks for later use.</li>
        <li>Terminate the nodes in the source zone.</li>
        <li>Create new root and data disks from snapshots created previously.</li>
        <li>Create instances for the nodes in the destination zone (using new root and data disks created in the previous step).</li>
        <li>Start up the master services on the new nodes.</li>
        <li>Once the new instances are up and services are running, the cluster will be available to accept jobs.</li>
      </ol>

    </li>
    <li>
      <span class="bld">Decommission one rack of DataNodes and TaskTrackers.</span> The nodes assigned to one of the racks in the source zone can be decommissioned by following these steps:
      <ol type="a">
        <li>Remove hostnames from the files enumerated in the previous Step 1, and add them to the <span class="code">dfs.hosts.exclude</span> and <span class="code">mapred.hosts.exclude</span> files.</li>
        <li>Run <span class="code">hadoop dfsadmin -refreshNodes</span> and <span class="code">hadoop mradmin -refreshNodes</span>. This sequence initiates the replication of data blocks into the destination zone. The DataNode decommissioning process is not instantaneous<sup><a href="#ftnt7" name="ftnt_ref7" id="ftnt_ref7">[7]</a></sup> and the status of the process on each node needs to be monitored (for example, via the web UI of the NameNode).</li>
        <li>Once a node reports a &ldquo;Decommissioned&rdquo; state, it may be safely terminated.<br></li>
      </ol>
    </li>
    <li><span class="bld">Decommission the other racks of DataNodes and TaskTrackers.</span> Once all nodes in <span class="code">rack 1</span> have been decommissioned, at least one copy of each block will exist on a DataNode in the destination zone. At this time, all instances in the <span class="code">rack 2</span> in the source zone can be decommissioned, as in the previous step.</li>
  </ol>

  <h4>Scenario 2: Simplifying management</h4>
  <p>This scenario presents how a migration can be performed with a single command, easing migration operations at the possible expense of longer cluster downtime. This process is nearly identical to the automatic migration of NameNode and JobTracker instances, as described in the previous scenario. <span class="code">gcutil moveinstances</span> migrates the entire cluster between zones.</p>

  <figure>
    <img src="/images/articles/managing-hadoop-clusters-on-gce/simplifying-management.png">
    <figcaption>Figure 2: Simplifying management for zone migration</figcaption>
  </figure>


  <h5>Approach</h5>
  <ol>
    <li><span class="bld">Migrate the cluster.</span> Use the <span class="code">gcutil moveinstances</span> command to clone all of the nodes in the cluster and their disks into the destination zone. For example, consider a small cluster with a single master node (<span class="code">hadoop-hm</span>) and ten worker nodes (<span class="code">hadoop-hw-000, &hellip;, hadoop-hw-009</span>). The following single call will move from the <span class="code">us-central2-a</span> zone and restart the cluster in<span class="code"> us-central1-a</span>:
    <p class="code">gcutil --project=&lt;project&gt; moveinstances hadoop-hm hadoop-hw-00\\d+ <span style="white-space:nowrap;">--source_zone=us-central2-a </span>--destination_zone=us-central1-a</p>
    </li>
  </ol>


  <h3>Disaster recovery</h3>
  <p>Catastrophic zone-wide failures are extremely rare, but it is always a good practice to account for the unexpected.<sup><a href="#ftnt8" name="ftnt_ref8" id="ftnt_ref8">[8]</a></sup> </p>

  <p>One way to address disaster recovery is to perform periodic snapshots of disks. Snapshots are geo-replicated and exist outside Google Compute Engine zones. They can be used, if necessary, to create new disks for replacement instances in a recovery zone.</p>

  <h5>Considerations</h5>

  <ol>
    <li>Disks are never lost if the zone in which they exist goes down. However, they will be unavailable until the zone is restored. Snapshotting is a technique that can accelerate the reconstitution of the cluster.</li>
  </ol>

  <figure>
    <img src="/images/articles/managing-hadoop-clusters-on-gce/persistent-disks.png">
    <figcaption>Figure 3: Persistent disks for disaster recovery</figcaption>
  </figure>


  <h5>Approach</h5>
  <ol>
    <li><span class="bld">Take frequent snapshots</span>. Establish a mechanism for creating regular snapshots<sup><a href="#ftnt9" name="ftnt_ref9" id="ftnt_ref9">[9]</a></sup> for persistent disks. It is strongly recommended that the cluster be quiescent when snapshots are made in order to reduce the potential for data inconsistency.</li>
    <li><span class="bld">Rebuild disks from snapshots</span>. In the event that the zone hosting the cluster becomes unavailable, new persistent disks can be created from the snapshots.</li>
    <li><span class="bld">Bring up replacement nodes</span>. New instances for all nodes in the cluster can be spun-up in a new zone using the new persistent disks.</li>
    <li><span class="bld">(Optional) Repopulate HDFS</span>. If persistent disks are not used (or snapshotted) on the DataNodes, the new HDFS must be (re)populated with data.</li>
  </ol>

  <a href="#" name="id.db8xgq3x77ts" id="id.db8xgq3x77ts"></a>
  <h4>Multi-zone cluster</h4>
  <p>Another possible option<sup><a href="#ftnt10" name="ftnt_ref10" id="ftnt_ref10">[10]</a></sup> is to deploy a cluster that straddles multiple zones. More specifically, in a second zone, an additional set of DataNodes and TaskTrackers are commissioned and, if applicable, NameNode and/or JobTracker standby nodes are placed.</p>

  <h5>Considerations</h5>
  <ol>
    <li>This pattern requires twice the number of DataNodes and TaskTrackers than would otherwise be deployed and requires substantial cross-zone communication.  While this pattern results in twice the processing capability, operational (instance, storage and network) expenses will increase and overall performance could be degraded, especially if data is accessed across zones.</li>
    <li>Some distributions provide high availability features for the NameNode and the JobTracker. See the High Availability section later in this paper for more information regarding their deployment.</li>
    <li>Other techniques such as rsync, NFS, or other distributed filesystems, can be used to synchronize NameNode directories.<sup><a href="#ftnt11" name="ftnt_ref11" id="ftnt_ref11">[11]</a></sup></li>
  </ol>

  <figure>
    <img src="/images/articles/managing-hadoop-clusters-on-gce/multi-zone-cluster.png">
    <figcaption>Figure 4: A multi-zone cluster for disaster recovery</figcaption>
  </figure>


  <h5>Approach</h5>
  <ol>
    <li><span class="bld">Distribute the cluster</span>. DataNodes and TaskTrackers are spread equally across both zones and fully commissioned. The DataNodes within a zone should identify with the same rack and each zone should use a different rack identifier, ensuring that at least one copy of each data block exists in the second zone.</li>
    <li><span class="bld">Failover the NameNode</span>. The NameNode can either be restored in the second zone automatically (for distributions providing this functionality) or manually, by creating a replacement instance (if one is not already running) and starting up the HDFS service.</li>
    <li><span class="bld">Failover the  JobTracker</span>. The JobTracker can either be restored in the second zone automatically (for distributions providing this functionality) or manually, by creating a replacement instance and starting up the service. Note that in the manual scenario, the JobTracker must be replaced with a new instance. Changing hostnames of the instance requires that the cluster configuration be updated and the TaskTrackers be restarted. So, unless otherwise required, simply reuse the same hostname when creating the new instance.</li>
  </ol>

  <h3>High availability</h3>
  <p>Traditionally, high availability of a Hadoop cluster is understood as the ability to recover from the removal of the active HDFS NameNode (for example, due either to unexpected failure) through the automatic activation of a standby. Some vendors provide this functionality as an add-on benefit to their Hadoop 1.x distribution. Hadoop 2.x incorporates several mechanisms such as <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithNFS.html">NFS shared storage</a> and <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">quorum-based journaling</a> to address this.<sup><a href="#ftnt12" name="ftnt_ref12" id="ftnt_ref12">[12]</a></sup></p>
  <p>The JobTracker in Hadoop 1.x and the YARN ResourceManager in Hadoop 2.x both must also be considered single points of failure. If the service crashes, MapReduce will be unavailable until it is restarted. While specific outcomes vary by distribution, in most cases, any jobs that are still running will be restarted rather than resumed. However, if the node itself crashes, it must be replaced manually, or a distribution that supports failover to a standby must be used.<sup><a href="#ftnt13" name="ftnt_ref13" id="ftnt_ref13">[13]</a></sup> </p>
  <p>While deploying a high availability Hadoop installation on Google Compute Engine is straightforward, certain features of the environment warrant additional attention.</p>

  <h4>Google Compute Engine Network DNS</h4>
  <p>One of the benefits of Google Compute Engine is the ability to address an instance on the network in any zone in any region by its user-provided hostname. Additionally, these names can be reused when instances are recycled (turning down one instance and spinning up another with the same name), which is convenient for deployments that want to rely on name-based addressing. It is also worth noting that there are no guarantees around internal IP address assignment. That instance hostnames can be reused dynamically is due to the fact that DNS entries are cleaned up almost immediately after an instance is terminated.</p>

  <p>Currently, Hadoop HDFS 2.0  is <a href="https://issues.apache.org/jira/browse/HDFS-4957">susceptible</a> to failure due to this behavior when configured to use the Quorum Journal Manager (QJM) for H/A. In particular, if the active NameNode instance is terminated, the automatic failover process will crash because it will fail to resolve that node’s hostname. One workaround is to add the NameNode hostnames and IP addresses to the <span class="code">/etc/hosts</span> files on each instance. This allows the automatic failover to succeed since the QJM appears to handle connection timeouts properly. Please note that this workaround creates somewhat of a maintenance issue since files need to be kept up to date whenever a new NameNode is spun up.</p>

  <h4>ZooKeeper</h4>
  <p><a href="http://zookeeper.apache.org/">Apache ZooKeeper</a> is used in Hadoop 2.0 to provide automatic failover capabilities for HDFS high availability using both NFS and the Quorum Journal Manager.  It&rsquo;s worth noting that special considerations need to made with regards to DNS when deploying ZooKeeper for Hadoop on Google Compute Engine.</p>
  <p>Currently, ZooKeeper resolves DNS names <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-338">once</a> at startup. This prevents the replacement of a member instance without rebooting the remaining members of the ensemble. So given the previous description of the behavior of Google Compute Engine DNS, it is worth noting that any time a node running ZooKeeper is rebooted or replaced, all other ZooKeeper services must be restarted. Failure to do so will leave the existing ensemble with one less member, and the new instance will be running what will amount to an orphaned ZooKeeper instance.</p>


  <h4>Straddling multiple zones</h4>
  <p>High availability can also be considered in terms of resilience of the cluster to zone failure. This might be applicable in scenarios where any interruption of long running jobs or pipelines might be detrimental and/or incur substantial costs. Consider the scenario where zone A is scheduled for maintenance earlier than <span class="code">zone B</span>. Given this knowledge, it is possible to construct a cluster spanning both zones such that it will continue to run in the event that either goes down.</p>


  <figure>
    <img src="/images/articles/managing-hadoop-clusters-on-gce/straddling-multiple-zones.png">
    <figcaption>Figure 5: Straddling multiple zones for high availability</figcaption>
  </figure>


  <ol>
    <li><span class="bld">Distribute the cluster</span>. As previously addressed, DataNodes and TaskTrackers are split equally across both zones and commissioned as part of the HDFS cluster; multiple HDFS racks are used to ensure data is fully replicated.</li>
    <li><span class="bld">Distribute the ZooKeeper ensemble</span>. The majority of ZooKeeper nodes must be deployed in the <span class="code">zone B</span> along with the <span class="ital">standby</span> NameNode and the <span class="ital">active</span> JobTracker.</li>
    <li><span class="bld">Manage the NameNode</span>. If <span class="code">zone A</span> becomes unavailable, the ZooKeeper ensemble still maintains a quorum and can automatically facilitate the failover and promotion of the standby NameNode to active. If instead, <span class="code">zone B</span> experiences an outage, the active NameNode will continue to run. The cluster, however, will not be able to sustain an active node failure as the ensemble has no quorum. Replacement ZooKeeper nodes must be added and the ensemble must be restarted.<sup><a href="#ftnt14" name="ftnt_ref14" id="ftnt_ref14">[14]</a></sup></li>
    <li><span class="bld">Manage the JobTracker</span>. The active JobTracker is deployed to <span class="code">zone B</span>, so that if <span class="code">zone A</span> becomes unavailable (as anticipated), any active jobs can continue running until completion. However, if the distribution relies on ZooKeeper to failover the JobTracker, or if this functionality is not supported by the distribution, a replacement must be brought up and any running jobs will likely require resubmission. It should be possible to augment this deployment with either option outlined in the <a href="#id.db8xgq3x77ts">multi-zone cluster</a> disaster recovery scenario.</li>
  </ol>

  <h2>Conclusion</h2>
  <p>Google Cloud Platform not only offers a high performance platform upon which to run Hadoop, but also provides features and tools that can assist in the maintenance of clusters across zones to keep business-critical jobs running in zone migration, disaster recovery, and high availability scenarios. Persistent disks, Google Cloud Storage, and the network infrastructure enable efficient data and instance migration; and <span class="code">gcutil</span> provides a rich set of commands to help accomplish cluster management tasks.</p>

  <h2>Additional resources</h2>
  <ul>
    <li><a href="https://github.com/GoogleCloudPlatform/solutions-google-compute-engine-cluster-for-hadoop">google-compute-engine-cluster-for-hadoop</a> &ndash; a sample application to assist in setting up Hadoop compute clusters and executing MapReduce tasks. Please note that this application does not perform any of the cluster management outlined in this paper.</li>
  </ul>


  <br>
  <hr>
  <p><a href="#ftnt_ref1" name="ftnt1" id="ftnt1">[1]</a> Google Compute Engine&rsquo;s fast virtual machine creation and sub-hour billing model enable customers to perform data processing using ad hoc clusters, without the overhead of managing resources constrained by timeboxed pricing.</p>
  <p><a href="#ftnt_ref2" name="ftnt2" id="ftnt2">[2]</a> Relaxing this constraint enables one to take advantage of the preferred approach of leveraging Google Cloud Storage as the durable repository.</p>
  <p><a href="#ftnt_ref3" name="ftnt3" id="ftnt3">[3]</a> One that provides no support for high availability features or functionality, such as Apache Hadoop 1.1.2</p>
  <p><a href="#ftnt_ref4" name="ftnt4" id="ftnt4">[4]</a> As of July 2013, up to 10TB of persistent disk storage can be mounted on standard instance types.</p>
  <p><a href="#ftnt_ref5" name="ftnt5" id="ftnt5">[5]</a> While it is not uncommon to ignore these files in most deployments, they are required in support of graceful commissioning and decommissioning of these nodes in the cluster. See the Hadoop <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a> and <a href="http://wiki.apache.org/hadoop/FAQ">FAQ</a> for more details.</p>
  <p><a href="#ftnt_ref6" name="ftnt6" id="ftnt6">[6]</a> Google Compute Engine instances are not named by internal IP address and are addressable by the hostname provided at creation time without the use of an additional name resolver.</p>
  <p><a href="#ftnt_ref7" name="ftnt7" id="ftnt7">[7]</a> Refer to the <a href="http://wiki.apache.org/hadoop/FAQ">Hadoop FAQ</a> &sect;3.6 for more details on decommissioning nodes.</p>
  <p><a href="#ftnt_ref8" name="ftnt8" id="ftnt8">[8]</a> <a href="/compute/docs/sla">Google Compute Engine Service Level Agreement</a></p>
  <p><a href="#ftnt_ref9" name="ftnt9" id="ftnt9">[9]</a> Refer to the <a href="/compute/docs/disks#snapshots">documentation</a> for more details on creating snapshots.</p>
  <p><a href="#ftnt_ref10" name="ftnt10" id="ftnt10">[10]</a> This option is presented for sake of completeness and is not typically recommended because it incurs additional costs and may impact performance.</p>
  <p><a href="#ftnt_ref11" name="ftnt11" id="ftnt11">[11]</a> Design and implementation of these mechanisms are beyond the scope of this document and should always be rigorously tested before use in production situations.</p>
  <p><a href="#ftnt_ref12" name="ftnt12" id="ftnt12">[12]</a> As of July 2013, Apache Hadoop 2.0 is still in alpha.</p>
  <p><a href="#ftnt_ref13" name="ftnt13" id="ftnt13">[13]</a> As of July 2013, Apache Hadoop does not support high availability for the JobTracker or YARN ResourceManager, although there is <a href="https://issues.apache.org/jira/browse/YARN-149">proposal</a> for the latter.</p>
  <p><a href="#ftnt_ref14" name="ftnt14" id="ftnt14">[14]</a> As of July 2013, while ZooKeeper 3.5 adds support for <a href="http://issues.apache.org/jira/browse/ZOOKEEPER-107">dynamic membership</a>, Hadoop 2.0.5-alpha uses ZooKeeper 3.4.2.</p>


</div>
<!-- /maia-main -->

    </div>
  </div>
</div>
