<!DOCTYPE html>
<html devsite="">
<head>
<title>Big Data Articles &amp; Tutorials — Google Cloud Platform</title>
<meta name="description" content="Read about technical big data articles and solutions with Google Cloud Platform. Big data articles unpack BigQuery data analytics, Hadoop clusters and more.">
<meta name="hide_page_heading" value="true">
<meta name="full_width" value="true">
<meta name="top_category" value="developers">
<meta name="subcategory" value="articles">
<meta name="viewport" content="initial-scale=1, minimum-scale=1, width=device-width">
<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link href="//fonts.googleapis.com/css?family=Open+Sans:400italic,300,400,600,700" rel="stylesheet">
<link href="/css/default.css" rel="stylesheet"><!--[if lt IE 9]>
    <link rel="stylesheet" media="screen" href='/css/cp-ie.css'>
    <![endif]-->
<script src="/js/floodlight.js">
</script>
</head>
<body>
<div id="maia-main" class="cp-article">
<div class="maia-cols">
<div class="maia-col-9">
<div>
<div style="float:right">
<div class="g-plusone"></div>
</div>
<h1 class="title">Découvrir les secrets de l'univers avec Google&nbsp;Compute&nbsp;Engine</h1>
</div>
<div class="cp-article-tutorial">
<h2>Organisme</h2>
<p>ATLAS est une expérience de physique des particules menée par le CERN à Genève (Suisse), dans son grand collisionneur de hadrons. Le détecteur ATLAS étudie la collision frontale de protons et d'ions lourds d'extrêmement haute énergie en vue de nouvelles découvertes. <a href="http://en.wikipedia.org/wiki/ATLAS_experiment">ATLAS</a> utilise un paradigme d'informatique en grille pour organiser ses ressources distribuées. Ce programme gère avec succès près de 200&nbsp;sites accueillant plus de 3&nbsp;000&nbsp;chercheurs dans le monde entier. Récemment, ATLAS a utilisé Google&nbsp;Compute&nbsp;Engine pour accroître sa capacité et pour satisfaire la demande liée à des charges de travail en augmentation constante. Ce projet a démontré que Google&nbsp;Compute&nbsp;Engine pouvait être intégré facilement et parfaitement dans la Grille de calcul d'ATLAS, et que la recherche en physique de production pouvait avoir lieu dans le cloud.</p>
<h2>Défi à relever</h2>
<p>Toutes les ressources disponibles de la Grille de calcul d'ATLAS fonctionnent à pleine capacité à longueur d'année. Selon l'équipe ATLAS, l'analyse des données d'observation au niveau des utilisateurs et des groupes, ainsi que les générateurs d'événements et les simulations de détecteurs, peuvent excéder un&nbsp;million de tâches par jour. Elle constate cependant que la demande dépasse souvent deux&nbsp;fois ce nombre, et qu'à plusieurs reprises en cours d'année, elle va bien au-delà de la capacité de la grille, qui s'élève à plus de 100&nbsp;000&nbsp;cœurs de processeurs. Un tel décalage peut exercer une pression sur le système au niveau de la planification et de la hiérarchisation des tâches, ce qui risque de ralentir la recherche. En résumé, le manque de ressources informatiques ralentit le rythme des découvertes scientifiques. C'est pourquoi les chercheurs d'ATLAS ont créé un projet de recherche et de développement visant à étudier la virtualisation et à déterminer si le cloud computing pouvait être utilisé pour gérer les pics de demande en ressources informatiques.</p>
<h2>Solution</h2>
<p>L'équipe ATLAS a commencé à utiliser Google&nbsp;Compute&nbsp;Engine durant l'automne&nbsp;2012. Le premier projet s'est concentré sur la construction de clusters d'analyse hautes performances semblables à ceux qu'utilise ATLAS pour l'analyse interactive et par lot d'ensembles de données volumineux issus d'expériences en physique des particules. "L'accès à Google&nbsp;Compute&nbsp;Engine nous a permis de construire et de tester de vastes clusters <a href="http://root.cern.ch/drupal/content/proof">PROOF</a> d'une capacité pouvant atteindre 1&nbsp;000&nbsp;travailleurs", explique le Dr&nbsp;Sergey&nbsp;Panitkin de Brookhaven National Lab, responsable en recherche et développement dans le cloud computing pour ATLAS. Ces clusters de traitement de données parallèles sont sensibles aux performances de l'infrastructure de stockage sous-jacente. Les configurations de type disque à mémoire seule, disque scratch et disque persistant furent toutes évaluées.</p>
<p>L'objectif du deuxième&nbsp;projet était d'accroître la capacité du système, d'où l'ajout de 4&nbsp;000&nbsp;cœurs de processeur qui furent structurés en tant que file d'attente <a href="http://research.cs.wisc.edu/htcondor/">PanDA</a> basée sur <a href="http://iopscience.iop.org/1742-6596/219/6/062028">HTCondor</a>, et intégrés de manière transparente à la Grille de calcul d'ATLAS. Ce cluster a été créé pour exécuter des simulations de charge de travail consommant de vastes ressources du processeur selon la méthode Monte-Carlo. L'objectif était de tester la stabilité à long terme d'un cluster basé sur le cloud et d'une taille similaire à celle d'un grand site de grille ATLAS (niveau&nbsp;2). L'une des conditions requises était qu'il devait être exécuté en tant que plate-forme de production complète, et pas seulement comme preuve de concept dans un contexte de type recherche et développement.</p>
<h2>Résultats</h2>
<p>L'équipe ATLAS a signalé que l'adaptabilité et les performances de l'infrastructure Google&nbsp;Compute&nbsp;Engine lui a permis d'exécuter des charges de travail à forte demande d'E/S à des niveaux sans précédent. C'était la première fois que les chercheurs avaient réussi à mener des tests à cette échelle. L'équipe mit en place un cluster d'analyse PROOF (exécutant exclusivement Google&nbsp;Compute&nbsp;Engine) auquel participèrent 500&nbsp;travailleurs, et capable de supporter un débit de 210&nbsp;millions de cycles par seconde. Cela lui permit d'observer des opportunités d'optimisation qui n'étaient pas évidentes lors de tests similaires effectués sur sa propre infrastructure.</p>
<p>En outre, le cluster de simulation a continué de fonctionner pendant près de deux&nbsp;mois dans le cadre de la Grille de calcul distribuée d'ATLAS. Il a ainsi enregistré plus de 5&nbsp;millions d'heures de processeur, effectué 458&nbsp;000&nbsp;tâches à forte intensité de calcul et traité environ 214&nbsp;millions d'événements. Le débit de pointe soutenu réalisé par le cluster atteignit 15&nbsp;000&nbsp;tâches par jour. "Notre expérience de Google&nbsp;Compute&nbsp;Engine fut très positive… et nous sommes convaincus que c'est l'infrastructure de cloud moderne qui peut servir de plate-forme stable et à hautes performances pour l'informatique scientifique", a conclu le Dr&nbsp;Panitkin.</p>
</div>
<!-- /maia-main --></div>
<div class="maia-col-3"><br>
<img src="//www.google.com/images/icons/product/feedback-16.png" class="g-app-icon" alt=""> <a href="javascript:void(0);" class="google-feedback">Commentaires sur ce document</a><br>
<br>
<!-- <img src="/images/articles/mapping-the-secrets/ATLAS-chrome-logo-blue-wh_lo.png" alt="Atlas Experiment"> -->
<p class="qte">"Le projet fut très stable au niveau du cloud. Google&nbsp;Compute&nbsp;Engine était d'une fiabilité à toute épreuve… aucune défaillance ne fut causée par un problème lié à GCE."</p>
<p class="qte-author">– Dr&nbsp;Sergey&nbsp;Panitkin, Brookhaven National Lab</p>
</div>
</div>
</div>
<script>
(function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><script src="//www.gstatic.com/feedback/api.js">
</script><script>
$('body').delegate('.google-feedback', 'click', function() {
      userfeedback.api.startFeedback({'productId': '94614'});
  });
</script><!-- Scripts to include both on Goro + Devsite --><script>
window.___gcfg = {
     lang: ''
   };
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script><!-- GTM implementation --><!-- Start dataLayer --><script>
dataLayer = [{
        'country': 'fr',
        'region': 'emea',
        'language': 'fr'
      }];
</script><!-- End dataLayer --><!-- Start Google Tag Manager --><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5CVQBG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5CVQBG');
</script><!-- End Google Tag Manager --><!-- Global JS scripts to load; path will depend on whether we're on devsite or Goro --><script src="/js/base.min.js">
</script><!-- Retina loader; Do not load if partners page because we need to wait until the Angular app runs --><script>
new lfl.system.RetinaLoader();
</script><!-- Secondary right-side scroll-nav --><script>
new lfl.ui.ScrollNav({});
</script>
</body>
</html>